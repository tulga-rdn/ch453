{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCRYKCqpNzir",
        "outputId": "33541101-6977-4c2b-cc0e-4912563b2d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-11-07 10:35:29--  https://repo.continuum.io/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c84f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh [following]\n",
            "--2022-11-07 10:35:29--  https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88867207 (85M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-py37_4.8.3-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-py37_4.8 100%[===================>]  84.75M  5.44MB/s    in 56s     \n",
            "\n",
            "2022-11-07 10:36:25 (1.52 MB/s) - ‘Miniconda3-py37_4.8.3-Linux-x86_64.sh’ saved [88867207/88867207]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2020.4.5.1=py37_0\n",
            "    - cffi==1.14.0=py37he30daa8_1\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.1=py37h7b6447c_0\n",
            "    - conda==4.8.3=py37_0\n",
            "    - cryptography==2.9.2=py37h1ba5d50_0\n",
            "    - idna==2.9=py_1\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.3=he6710b0_1\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_1\n",
            "    - openssl==1.1.1g=h7b6447c_0\n",
            "    - pip==20.0.2=py37_3\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.20=py_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.7=hcff3b4d_5\n",
            "    - readline==8.0=h7b6447c_0\n",
            "    - requests==2.23.0=py37_0\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==46.4.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h62c20be_1\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.46.0=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.5=h7b6447c_0\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2020.4.5.1-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37he30daa8_1\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.3-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.1-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.9.2-py37h1ba5d50_0\n",
            "  idna               pkgs/main/noarch::idna-2.9-py_1\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_1\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_1\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1g-h7b6447c_0\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_3\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.20-py_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.7-hcff3b4d_5\n",
            "  readline           pkgs/main/linux-64::readline-8.0-h7b6447c_0\n",
            "  requests           pkgs/main/linux-64::requests-2.23.0-py37_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-46.4.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h62c20be_1\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.46.0-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.5-h7b6447c_0\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "\n",
            "real\t0m18.958s\n",
            "user\t0m12.942s\n",
            "sys\t0m3.640s\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit==2020.09.2\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.74.0               |   py37he5a615d_2         335 KB  conda-forge\n",
            "    boost-cpp-1.74.0           |       h9359b55_0        16.4 MB  conda-forge\n",
            "    bzip2-1.0.8                |       h516909a_3         398 KB  conda-forge\n",
            "    ca-certificates-2022.9.24  |       ha878542_0         150 KB  conda-forge\n",
            "    cairo-1.16.0               |    h3fc0475_1005         1.5 MB  conda-forge\n",
            "    certifi-2022.9.24          |     pyhd8ed1ab_0         155 KB  conda-forge\n",
            "    conda-22.9.0               |   py37h89c1867_1         960 KB  conda-forge\n",
            "    cycler-0.11.0              |     pyhd8ed1ab_0          10 KB  conda-forge\n",
            "    fontconfig-2.13.1          |    h7e3eb15_1002         324 KB  conda-forge\n",
            "    freetype-2.10.4            |       h7ca028e_0         912 KB  conda-forge\n",
            "    glib-2.68.2                |       h36276a3_0         3.0 MB\n",
            "    icu-67.1                   |       he1b5a44_0        12.9 MB  conda-forge\n",
            "    jpeg-9d                    |       h36c2ea0_0         264 KB  conda-forge\n",
            "    kiwisolver-1.3.1           |   py37hc928c03_0          86 KB  conda-forge\n",
            "    lcms2-2.11                 |       hcbb858e_1         434 KB  conda-forge\n",
            "    libblas-3.9.0              |       8_openblas          11 KB  conda-forge\n",
            "    libcblas-3.9.0             |       8_openblas          11 KB  conda-forge\n",
            "    libgfortran-ng-7.5.0       |      h14aa051_20          23 KB  conda-forge\n",
            "    libgfortran4-7.5.0         |      h14aa051_20         1.2 MB  conda-forge\n",
            "    libiconv-1.16              |       h516909a_0         1.4 MB  conda-forge\n",
            "    liblapack-3.9.0            |       8_openblas          11 KB  conda-forge\n",
            "    libopenblas-0.3.12         |pthreads_hb3c22a3_1         8.2 MB  conda-forge\n",
            "    libpng-1.6.37              |       h21135ba_2         306 KB  conda-forge\n",
            "    libtiff-4.1.0              |       hc3755c2_3         568 KB  conda-forge\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    libxml2-2.9.10             |       h68273f3_2         1.3 MB  conda-forge\n",
            "    lz4-c-1.9.2                |       he1b5a44_3         203 KB  conda-forge\n",
            "    matplotlib-base-3.3.4      |   py37h62a2d02_0         5.1 MB\n",
            "    numpy-1.19.4               |   py37h7e9df27_1         5.2 MB  conda-forge\n",
            "    olefile-0.46               |     pyh9f0ad1d_1          32 KB  conda-forge\n",
            "    openssl-1.1.1h             |       h516909a_0         2.1 MB  conda-forge\n",
            "    pandas-1.1.4               |   py37h10a2094_0        10.5 MB  conda-forge\n",
            "    pcre-8.44                  |       he1b5a44_0         261 KB  conda-forge\n",
            "    pillow-8.2.0               |   py37he98fc37_0         622 KB\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h36c2ea0_1001           5 KB  conda-forge\n",
            "    pycairo-1.20.0             |   py37h01af8b0_1          77 KB  conda-forge\n",
            "    pyparsing-3.0.9            |     pyhd8ed1ab_0          79 KB  conda-forge\n",
            "    python-dateutil-2.8.2      |     pyhd8ed1ab_0         240 KB  conda-forge\n",
            "    python_abi-3.7             |          2_cp37m           4 KB  conda-forge\n",
            "    pytz-2022.6                |     pyhd8ed1ab_0         235 KB  conda-forge\n",
            "    rdkit-2020.09.2            |   py37h713bca6_0        25.7 MB  conda-forge\n",
            "    reportlab-3.5.55           |   py37hbf381c1_0         2.4 MB  conda-forge\n",
            "    sqlalchemy-1.3.20          |   py37h8f50634_0         1.8 MB  conda-forge\n",
            "    toolz-0.12.0               |     pyhd8ed1ab_0          48 KB  conda-forge\n",
            "    tornado-6.1                |   py37h4abf009_0         645 KB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.12         |       h36c2ea0_0         919 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    zstd-1.4.5                 |       h6597ccf_2         712 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       108.8 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.74.0-py37he5a615d_2\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.74.0-h9359b55_0\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h516909a_3\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-h3fc0475_1005\n",
            "  cycler             conda-forge/noarch::cycler-0.11.0-pyhd8ed1ab_0\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h7e3eb15_1002\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.4-h7ca028e_0\n",
            "  glib               pkgs/main/linux-64::glib-2.68.2-h36276a3_0\n",
            "  icu                conda-forge/linux-64::icu-67.1-he1b5a44_0\n",
            "  jpeg               conda-forge/linux-64::jpeg-9d-h36c2ea0_0\n",
            "  kiwisolver         conda-forge/linux-64::kiwisolver-1.3.1-py37hc928c03_0\n",
            "  lcms2              conda-forge/linux-64::lcms2-2.11-hcbb858e_1\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-8_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-8_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.5.0-h14aa051_20\n",
            "  libgfortran4       conda-forge/linux-64::libgfortran4-7.5.0-h14aa051_20\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.16-h516909a_0\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-8_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.12-pthreads_hb3c22a3_1\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-h21135ba_2\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.1.0-hc3755c2_3\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  libxml2            conda-forge/linux-64::libxml2-2.9.10-h68273f3_2\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.9.2-he1b5a44_3\n",
            "  matplotlib-base    pkgs/main/linux-64::matplotlib-base-3.3.4-py37h62a2d02_0\n",
            "  numpy              conda-forge/linux-64::numpy-1.19.4-py37h7e9df27_1\n",
            "  olefile            conda-forge/noarch::olefile-0.46-pyh9f0ad1d_1\n",
            "  pandas             conda-forge/linux-64::pandas-1.1.4-py37h10a2094_0\n",
            "  pcre               conda-forge/linux-64::pcre-8.44-he1b5a44_0\n",
            "  pillow             pkgs/main/linux-64::pillow-8.2.0-py37he98fc37_0\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.20.0-py37h01af8b0_1\n",
            "  pyparsing          conda-forge/noarch::pyparsing-3.0.9-pyhd8ed1ab_0\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.2-pyhd8ed1ab_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-2_cp37m\n",
            "  pytz               conda-forge/noarch::pytz-2022.6-pyhd8ed1ab_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2020.09.2-py37h713bca6_0\n",
            "  reportlab          conda-forge/linux-64::reportlab-3.5.55-py37hbf381c1_0\n",
            "  sqlalchemy         conda-forge/linux-64::sqlalchemy-1.3.20-py37h8f50634_0\n",
            "  toolz              conda-forge/noarch::toolz-0.12.0-pyhd8ed1ab_0\n",
            "  tornado            conda-forge/linux-64::tornado-6.1-py37h4abf009_0\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.12-h36c2ea0_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.5-h6597ccf_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2022.9.24-ha878542_0\n",
            "  certifi            pkgs/main/linux-64::certifi-2020.4.5.~ --> conda-forge/noarch::certifi-2022.9.24-pyhd8ed1ab_0\n",
            "  conda                       pkgs/main::conda-4.8.3-py37_0 --> conda-forge::conda-22.9.0-py37h89c1867_1\n",
            "  openssl              pkgs/main::openssl-1.1.1g-h7b6447c_0 --> conda-forge::openssl-1.1.1h-h516909a_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\n",
            "real\t6m6.344s\n",
            "user\t5m39.600s\n",
            "sys\t0m8.453s\n"
          ]
        }
      ],
      "source": [
        "#Install miniconda\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-py37_4.8.3-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-py37_4.8.3-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# Install RDKit\n",
        "!time conda install -q -y -c conda-forge rdkit==2020.09.2\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsFAHJglN_xV",
        "outputId": "726019a4-5aaf-461b-c4f5-acba508f660e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('drive/MyDrive/CH453_Assignment2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RcPfmamEuho",
        "outputId": "4ce4ac02-883c-40a3-e5db-ab41e7714ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2022.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.5 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rdkit) (1.21.6)\n",
            "Installing collected packages: rdkit\n",
            "Successfully installed rdkit-2022.9.1\n"
          ]
        }
      ],
      "source": [
        "pip install rdkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbCGLOAHEtiw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWMaaeUpNmi8"
      },
      "outputs": [],
      "source": [
        "# Problem 1\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I52OLwYBQbRG"
      },
      "outputs": [],
      "source": [
        "class Prob1Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, smi_list):\n",
        "        super().__init__()\n",
        "\n",
        "        self.smi_list = smi_list\n",
        "\n",
        "        ##########Implement Here!##########\n",
        "        '''\n",
        "        Get the longest length of smiles in the given smi_list\n",
        "        '''\n",
        "        self.max_length = len(max(smi_list, key = len))\n",
        "        ###################################\n",
        "\n",
        "        self._set_c_to_i()\n",
        "        self.vec_dim = self._get_num_char()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smi_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        return a dict of {\"input\": input, \"output\": output},\n",
        "        where input is a numpy array of encoded smiles and\n",
        "        output is a numpy array of corresponding tpsa value.\n",
        "        use self._encode_smi and self._get_tpsa.\n",
        "        '''\n",
        "        sample = dict()\n",
        "        ##########Implement Here!##########\n",
        "        sample = {\n",
        "                \"input\": self._encode_smi(self.smi_list[idx]),\n",
        "                \"output\": self._get_tpsa(self.smi_list[idx])\n",
        "        }\n",
        "        ###################################\n",
        "        return sample\n",
        "\n",
        "    def _set_c_to_i(self):\n",
        "        '''\n",
        "        Obtain c_to_i dictionary from smi_list.\n",
        "        We'll use the characters in self.smi_list, and auxiliary character 'X'.\n",
        "        '''\n",
        "        c_to_i = dict()\n",
        "        ##########Implement Here!##########\n",
        "        whole_char = list(reduce(lambda x, y: x | y, \\\n",
        "                [set(smi) for smi in self.smi_list]))\n",
        "        whole_char.append('X') #### auxiliary character\n",
        "        c_to_i = {c: i for i, c in enumerate(whole_char)}\n",
        "        self.c_to_i = c_to_i\n",
        "        ###################################\n",
        "        self.c_to_i = c_to_i\n",
        "\n",
        "    def _get_num_char(self):\n",
        "        return len(getattr(self, \"c_to_i\", dict()))\n",
        "\n",
        "    def _encode_smi(self, smi):\n",
        "        '''\n",
        "        return a numpy array of encoded smiles which dimension is [max_length, num_char].\n",
        "        use self._collate_smi and self._get_one_hot_vector.\n",
        "        Hint:\n",
        "          1. First obtain collated smiles\n",
        "          2. Then change each character of collated smiles to int by c_to_i\n",
        "          3. Finally make one-hot vector from each int\n",
        "        '''\n",
        "        assert self._get_num_char() > 0, \"c_to_i undefined\"\n",
        "        ##########Implement Here!##########\n",
        "        return np.array([self._get_one_hot_vector(self.c_to_i[c], self.vec_dim) \\\n",
        "                for c in self._collate_smi(smi)]) # [N, vec_dim]\n",
        "        ###################################\n",
        "\n",
        "    def _collate_smi(self, smi, pad_char='X'):\n",
        "        '''\n",
        "        return a collated smiles (str) which is padded with pad_char with length self.max_length.\n",
        "        '''\n",
        "        assert len(smi) <= self.max_length, \"given smiles longer than the max_length\"\n",
        "        ##########Implement Here!##########\n",
        "        return smi.ljust(self.max_length, 'X')\n",
        "        ###################################\n",
        "\n",
        "    def _get_one_hot_vector(self, idx, vec_dim):\n",
        "        '''\n",
        "        return a numpy array of one-hot vector: length vec_dim where idx-th index is 1.\n",
        "        '''\n",
        "        ##########Implement Here!##########\n",
        "        return np.eye(vec_dim)[idx]\n",
        "        ###################################\n",
        "\n",
        "    def _get_tpsa(self, smi):\n",
        "        '''\n",
        "        return a numpy array of TPSA of given smiles.\n",
        "        '''\n",
        "        ##########Implement Here!##########\n",
        "\n",
        "        ###################################\n",
        "        return np.array([CalcTPSA(Chem.MolFromSmiles(smi))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfsYe8PAQg60"
      },
      "outputs": [],
      "source": [
        "def random_splitter(dataset, train_ratio, validation_ratio, test_ratio):\n",
        "    import random\n",
        "    import copy\n",
        "    assert train_ratio + validation_ratio + test_ratio == 1.0\n",
        "    N = len(dataset)\n",
        "    all_idx = list(range(N))\n",
        "    random.shuffle(all_idx)\n",
        "\n",
        "    train_idx = all_idx[:int(train_ratio * N)]\n",
        "    valid_idx = all_idx[int(train_ratio * N):int(validation_ratio * N)\n",
        "                                                   + int(train_ratio * N)]\n",
        "    test_idx = all_idx[int(validation_ratio * N) + int(train_ratio * N):]\n",
        "    train_dataset = copy.deepcopy(dataset)\n",
        "    valid_dataset = copy.deepcopy(dataset)\n",
        "    test_dataset = copy.deepcopy(dataset)\n",
        "    train_dataset.smi_list = [dataset.smi_list[i] for i in train_idx]\n",
        "    valid_dataset.smi_list = [dataset.smi_list[i] for i in valid_idx]\n",
        "    test_dataset.smi_list =  [dataset.smi_list[i] for i in  test_idx]\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN5kEpmMQk7i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LinearRegressor(nn.Module):\n",
        "    def __init__(self, embedding_dim=64, num_hidden_layers=10, n_char=43, max_len=64):\n",
        "        super(LinearRegressor, self).__init__()\n",
        "        ###############################\n",
        "        ######## Implement Here #######\n",
        "        ###############################\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.n_char = n_char\n",
        "        self.max_len = max_len\n",
        "\n",
        "        hidden_layers = []\n",
        "        for _ in range(num_hidden_layers):\n",
        "            hidden_layers.append(\n",
        "                nn.Linear(\n",
        "                    embedding_dim*max_len,\n",
        "                    embedding_dim*max_len,\n",
        "                ) # in_channels, out_channels,\n",
        "            )\n",
        "        self.hidden_layers = nn.ModuleList(hidden_layers)\n",
        "        self.fc = nn.Linear(embedding_dim*max_len, 1)\n",
        "        self.embedding = nn.Linear(n_char, embedding_dim, bias=False)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        retval = None\n",
        "        ###############################\n",
        "        ######## Implement Here #######\n",
        "        ###############################\n",
        "        retval = x\n",
        "        # Embedding\n",
        "        retval = self.embedding(retval)\n",
        "        #retval = retval.permute((0, 2, 1))\n",
        "        retval = retval.reshape(retval.size(0), -1)\n",
        "        # Convolution and activation\n",
        "        for layer in self.hidden_layers:\n",
        "            retval = layer(retval)\n",
        "            retval = self.activation(retval)\n",
        "        retval = self.fc(retval)\n",
        "        return retval                         # [B x 1]\n",
        "\n",
        "class ConvRegressor(nn.Module):\n",
        "    def __init__(self, n_channel=64, num_conv_layers=10, kernel_size=3, n_char=43, max_len=64, stride=1):\n",
        "        super(ConvRegressor, self).__init__()\n",
        "        ###############################\n",
        "        ######## Implement Here #######\n",
        "        ###############################\n",
        "        self.n_channel = n_channel\n",
        "        self.num_conv_layers = num_conv_layers\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.n_char = n_char\n",
        "\n",
        "        conv_layers = []\n",
        "        for _ in range(num_conv_layers):\n",
        "            conv_layers.append(\n",
        "                nn.Conv1d(\n",
        "                    n_channel,\n",
        "                    n_channel,\n",
        "                    kernel_size,\n",
        "                    stride=1,\n",
        "                    padding=1\n",
        "                ) # in_channels, out_channels, kernel_size, stride, padding\n",
        "            )\n",
        "\n",
        "        self.conv_layers = nn.ModuleList(conv_layers)\n",
        "        self.fc = nn.Linear(n_channel*max_len, 1)\n",
        "        self.embedding = nn.Linear(n_char, n_channel, bias=False)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        retval = x                                # [B x max_len x n_char]\n",
        "        # Embedding\n",
        "        retval = self.embedding(retval)           # [B x max_len x n_channel]\n",
        "        retval = retval.permute((0, 2, 1))        # [B x n_channel x max_len]\n",
        "        # Convolution and activation\n",
        "        for layer in self.conv_layers:\n",
        "            retval = layer(retval)\n",
        "            retval = self.activation(retval)\n",
        "        retval = retval.view(retval.size(0), -1)\n",
        "        retval = self.fc(retval)                  # [B x 1]\n",
        "        return retval   # [B x 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6kS4Bu9l2n_"
      },
      "outputs": [],
      "source": [
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTKC6jGlqIGt"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8QS4sB6Qoz7",
        "outputId": "9f17ee8a-08d3-428e-80a8-8f71f3444f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv model was used.\n",
            "1th epoch,\n",
            "\ttraining loss: 3465.14533\n",
            "\tval loss: 880.06713\n",
            "\tepoch time: 17.995\n",
            "2th epoch,\n",
            "\ttraining loss: 750.16978\n",
            "\tval loss: 510.28950\n",
            "\tepoch time: 11.542\n",
            "3th epoch,\n",
            "\ttraining loss: 332.75378\n",
            "\tval loss: 226.23178\n",
            "\tepoch time: 10.121\n",
            "4th epoch,\n",
            "\ttraining loss: 156.16331\n",
            "\tval loss: 110.86150\n",
            "\tepoch time: 10.002\n",
            "5th epoch,\n",
            "\ttraining loss: 81.49003\n",
            "\tval loss: 72.93836\n",
            "\tepoch time: 10.920\n",
            "6th epoch,\n",
            "\ttraining loss: 57.95303\n",
            "\tval loss: 54.14083\n",
            "\tepoch time: 10.234\n",
            "7th epoch,\n",
            "\ttraining loss: 47.97540\n",
            "\tval loss: 48.62931\n",
            "\tepoch time: 10.130\n",
            "8th epoch,\n",
            "\ttraining loss: 40.61791\n",
            "\tval loss: 40.22609\n",
            "\tepoch time: 11.511\n",
            "9th epoch,\n",
            "\ttraining loss: 34.69605\n",
            "\tval loss: 34.24821\n",
            "\tepoch time: 10.234\n",
            "10th epoch,\n",
            "\ttraining loss: 29.35376\n",
            "\tval loss: 27.94059\n",
            "\tepoch time: 10.020\n",
            "11th epoch,\n",
            "\ttraining loss: 25.64314\n",
            "\tval loss: 23.37036\n",
            "\tepoch time: 10.147\n",
            "12th epoch,\n",
            "\ttraining loss: 22.19492\n",
            "\tval loss: 25.69502\n",
            "\tepoch time: 10.097\n",
            "13th epoch,\n",
            "\ttraining loss: 17.94430\n",
            "\tval loss: 16.68287\n",
            "\tepoch time: 10.145\n",
            "14th epoch,\n",
            "\ttraining loss: 14.64131\n",
            "\tval loss: 13.15129\n",
            "\tepoch time: 10.113\n",
            "15th epoch,\n",
            "\ttraining loss: 11.96418\n",
            "\tval loss: 10.93791\n",
            "\tepoch time: 12.030\n",
            "16th epoch,\n",
            "\ttraining loss: 10.32812\n",
            "\tval loss: 9.31074\n",
            "\tepoch time: 10.522\n",
            "17th epoch,\n",
            "\ttraining loss: 8.30465\n",
            "\tval loss: 7.59748\n",
            "\tepoch time: 10.463\n",
            "18th epoch,\n",
            "\ttraining loss: 6.84360\n",
            "\tval loss: 7.44130\n",
            "\tepoch time: 10.403\n",
            "19th epoch,\n",
            "\ttraining loss: 5.86788\n",
            "\tval loss: 6.27884\n",
            "\tepoch time: 10.296\n",
            "20th epoch,\n",
            "\ttraining loss: 5.00167\n",
            "\tval loss: 4.74803\n",
            "\tepoch time: 10.370\n",
            "21th epoch,\n",
            "\ttraining loss: 4.59106\n",
            "\tval loss: 4.18730\n",
            "\tepoch time: 11.098\n",
            "22th epoch,\n",
            "\ttraining loss: 4.15743\n",
            "\tval loss: 4.16584\n",
            "\tepoch time: 13.033\n",
            "23th epoch,\n",
            "\ttraining loss: 3.46077\n",
            "\tval loss: 3.45912\n",
            "\tepoch time: 10.376\n",
            "24th epoch,\n",
            "\ttraining loss: 3.13605\n",
            "\tval loss: 2.96526\n",
            "\tepoch time: 10.279\n",
            "25th epoch,\n",
            "\ttraining loss: 2.86272\n",
            "\tval loss: 3.16163\n",
            "\tepoch time: 10.294\n",
            "26th epoch,\n",
            "\ttraining loss: 2.79603\n",
            "\tval loss: 2.97984\n",
            "\tepoch time: 10.356\n",
            "27th epoch,\n",
            "\ttraining loss: 2.37581\n",
            "\tval loss: 2.36591\n",
            "\tepoch time: 10.260\n",
            "28th epoch,\n",
            "\ttraining loss: 2.37708\n",
            "\tval loss: 3.26106\n",
            "\tepoch time: 12.780\n",
            "29th epoch,\n",
            "\ttraining loss: 2.27114\n",
            "\tval loss: 2.26818\n",
            "\tepoch time: 10.658\n",
            "30th epoch,\n",
            "\ttraining loss: 1.89800\n",
            "\tval loss: 2.18760\n",
            "\tepoch time: 13.322\n",
            "31th epoch,\n",
            "\ttraining loss: 1.80991\n",
            "\tval loss: 1.65825\n",
            "\tepoch time: 12.486\n",
            "32th epoch,\n",
            "\ttraining loss: 1.67184\n",
            "\tval loss: 1.44540\n",
            "\tepoch time: 10.587\n",
            "33th epoch,\n",
            "\ttraining loss: 1.72568\n",
            "\tval loss: 1.51615\n",
            "\tepoch time: 10.626\n",
            "34th epoch,\n",
            "\ttraining loss: 1.38275\n",
            "\tval loss: 1.17160\n",
            "\tepoch time: 12.038\n",
            "35th epoch,\n",
            "\ttraining loss: 1.27157\n",
            "\tval loss: 1.11762\n",
            "\tepoch time: 10.238\n",
            "36th epoch,\n",
            "\ttraining loss: 1.16401\n",
            "\tval loss: 1.09738\n",
            "\tepoch time: 10.127\n",
            "37th epoch,\n",
            "\ttraining loss: 1.08285\n",
            "\tval loss: 1.20300\n",
            "\tepoch time: 10.178\n",
            "38th epoch,\n",
            "\ttraining loss: 1.16603\n",
            "\tval loss: 1.31570\n",
            "\tepoch time: 10.124\n",
            "39th epoch,\n",
            "\ttraining loss: 1.01042\n",
            "\tval loss: 0.81522\n",
            "\tepoch time: 10.087\n",
            "40th epoch,\n",
            "\ttraining loss: 0.93933\n",
            "\tval loss: 0.93260\n",
            "\tepoch time: 11.630\n",
            "41th epoch,\n",
            "\ttraining loss: 0.93050\n",
            "\tval loss: 0.74893\n",
            "\tepoch time: 10.100\n",
            "42th epoch,\n",
            "\ttraining loss: 1.14286\n",
            "\tval loss: 0.90538\n",
            "\tepoch time: 10.054\n",
            "43th epoch,\n",
            "\ttraining loss: 0.86891\n",
            "\tval loss: 0.68505\n",
            "\tepoch time: 10.205\n",
            "44th epoch,\n",
            "\ttraining loss: 0.71326\n",
            "\tval loss: 0.60079\n",
            "\tepoch time: 10.117\n",
            "45th epoch,\n",
            "\ttraining loss: 0.70219\n",
            "\tval loss: 0.82733\n",
            "\tepoch time: 10.190\n",
            "46th epoch,\n",
            "\ttraining loss: 0.77958\n",
            "\tval loss: 1.10804\n",
            "\tepoch time: 10.176\n",
            "47th epoch,\n",
            "\ttraining loss: 0.72595\n",
            "\tval loss: 0.51705\n",
            "\tepoch time: 11.557\n",
            "48th epoch,\n",
            "\ttraining loss: 0.67669\n",
            "\tval loss: 0.50677\n",
            "\tepoch time: 10.092\n",
            "49th epoch,\n",
            "\ttraining loss: 0.62551\n",
            "\tval loss: 0.58448\n",
            "\tepoch time: 10.205\n",
            "50th epoch,\n",
            "\ttraining loss: 0.64830\n",
            "\tval loss: 0.47687\n",
            "\tepoch time: 10.133\n",
            "----------\n",
            "Train Finished.\n",
            "Training time: 540.75s\n",
            "The best epoch: 50\n",
            "The best val loss: 0.4768656005859375\n"
          ]
        }
      ],
      "source": [
        "# 1. Load data and preprocessing\n",
        "file_path = 'assignment_2_smiles.txt'\n",
        "with open(file_path, 'r') as f:\n",
        "    smi_list = [l.strip() for l in f.readlines()]\n",
        "\n",
        "# Define Dataset and Dataloader\n",
        "dataset = Prob1Dataset(smi_list)\n",
        "train_dataset, valid_dataset, _ = \\\n",
        "        random_splitter(dataset, 0.9, 0.1, 0.0)\n",
        "len_max = dataset.max_length\n",
        "char_len = dataset.vec_dim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "data_loaders = {}\n",
        "data_loaders['train'] = DataLoader(train_dataset, batch_size=128, shuffle = True)\n",
        "data_loaders['val'] = DataLoader(valid_dataset, batch_size=128, shuffle = False)\n",
        "\n",
        "# 2. Train model\n",
        "# Setting learning parameters\n",
        "num_epoch = 50\n",
        "loss_fn = nn.MSELoss(reduction='sum')\n",
        "lr = 1e-4\n",
        "\n",
        "# prepare model\n",
        "# model_architecture = 'Linear'\n",
        "model_architecture = 'Conv'\n",
        "\n",
        "if model_architecture == 'Linear':\n",
        "    model = LinearRegressor(embedding_dim=64, num_hidden_layers=10, n_char=char_len, max_len=len_max)\n",
        "elif model_architecture == 'Conv':\n",
        "    model = ConvRegressor(n_channel=64, num_conv_layers=10, kernel_size=3, n_char=char_len, max_len=len_max)\n",
        "print(f'{model_architecture} model was used.')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Model training\n",
        "import time, copy\n",
        "model.cuda()\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "best_val_loss = 1e6\n",
        "train_start = time.time()\n",
        "for i in range(num_epoch):\n",
        "    since = time.time()\n",
        "    # 2-1. Training phase\n",
        "    model.train()\n",
        "    train_loss_list = []\n",
        "    for batch_idx, batch in enumerate(data_loaders['train']):\n",
        "        ###############################\n",
        "        ######## Implement Here #######\n",
        "        ###############################\n",
        "        x_batch = batch[\"input\"].float().to(device)\n",
        "        y_batch = batch[\"output\"].float().to(device)\n",
        "\n",
        "        y_pred = model(x_batch)\n",
        "\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        train_loss_list.append(copy.deepcopy(loss.data.cpu().numpy()))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_train_avg_loss = \\\n",
        "            np.sum(np.array(train_loss_list))/len(data_loaders['train'].dataset)\n",
        "    train_loss_history.append(epoch_train_avg_loss)\n",
        "\n",
        "    # 2-2. Validation phase\n",
        "    model.eval()\n",
        "    val_loss_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(data_loaders['val']):\n",
        "            ###############################\n",
        "            ######## Implement Here #######\n",
        "            ###############################\n",
        "            x_batch = batch[\"input\"].float().to(device)\n",
        "            y_batch = batch[\"output\"].float().to(device)\n",
        "\n",
        "            y_pred = model(x_batch)\n",
        "\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            val_loss_list.append(loss.data.cpu().numpy())\n",
        "\n",
        "    epoch_val_avg_loss = \\\n",
        "            np.sum(np.array(val_loss_list))/len(data_loaders['val'].dataset)\n",
        "    val_loss_history.append(epoch_val_avg_loss)\n",
        "\n",
        "    if epoch_val_avg_loss < best_val_loss:\n",
        "        best_epoch = i+1\n",
        "        best_val_loss = epoch_val_avg_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # 2-3. print the result\n",
        "    end = time.time()\n",
        "    print(f'{i+1}th epoch,')\n",
        "    print(f'\\ttraining loss: {epoch_train_avg_loss:.5f}')\n",
        "    print(f'\\tval loss: {epoch_val_avg_loss:.5f}')\n",
        "    print(f'\\tepoch time: {end-since:.3f}')\n",
        "\n",
        "save_dir = 'assignment_2_models'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "torch.save(best_model_wts, f'{save_dir}/Best_{model_architecture}_{str(best_epoch)}.pt')\n",
        "print('-'*10)\n",
        "print('Train Finished.')\n",
        "print(f'Training time: {time.time()-train_start:.2f}s')\n",
        "print(f'The best epoch: {best_epoch}')\n",
        "print(f'The best val loss: {best_val_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "w9cduSB5QuMn",
        "outputId": "9362da7a-ae96-476c-cbf9-ae293d3cfc10"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 4.Plot the loss histories\n",
        "import matplotlib.pyplot as plt\n",
        "x_axis = np.arange(num_epoch)\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(x_axis, train_loss_history, label='train loss')\n",
        "ax.plot(x_axis, val_loss_history, label='val loss')\n",
        "ax.set_xlabel('num epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Loss History')\n",
        "ax.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6O2E9iDGYht",
        "outputId": "3a1c148d-efa0-416f-82ba-f630e1dd53a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear model was used.\n",
            "1th epoch,\n",
            "\ttraining loss: 852.36817\n",
            "\tval loss: 228.83691\n",
            "\tepoch time: 31.287\n",
            "2th epoch,\n",
            "\ttraining loss: 164.55849\n",
            "\tval loss: 108.34253\n",
            "\tepoch time: 31.642\n",
            "3th epoch,\n",
            "\ttraining loss: 79.70447\n",
            "\tval loss: 74.63292\n",
            "\tepoch time: 31.400\n",
            "4th epoch,\n",
            "\ttraining loss: 73.64092\n",
            "\tval loss: 78.00728\n",
            "\tepoch time: 31.397\n",
            "5th epoch,\n",
            "\ttraining loss: 39.14536\n",
            "\tval loss: 52.81660\n",
            "\tepoch time: 31.477\n",
            "6th epoch,\n",
            "\ttraining loss: 70.36649\n",
            "\tval loss: 57.49474\n",
            "\tepoch time: 31.408\n",
            "7th epoch,\n",
            "\ttraining loss: 31.59529\n",
            "\tval loss: 47.02222\n",
            "\tepoch time: 31.633\n",
            "8th epoch,\n",
            "\ttraining loss: 20.73062\n",
            "\tval loss: 50.12559\n",
            "\tepoch time: 31.577\n",
            "9th epoch,\n",
            "\ttraining loss: 177.62322\n",
            "\tval loss: 460.65584\n",
            "\tepoch time: 31.428\n",
            "10th epoch,\n",
            "\ttraining loss: 52.97769\n",
            "\tval loss: 45.03043\n",
            "\tepoch time: 31.996\n",
            "11th epoch,\n",
            "\ttraining loss: 16.95484\n",
            "\tval loss: 50.88469\n",
            "\tepoch time: 31.474\n",
            "12th epoch,\n",
            "\ttraining loss: 10.88293\n",
            "\tval loss: 37.09796\n",
            "\tepoch time: 31.411\n",
            "13th epoch,\n",
            "\ttraining loss: 7.18123\n",
            "\tval loss: 38.75133\n",
            "\tepoch time: 31.404\n",
            "14th epoch,\n",
            "\ttraining loss: 6.14393\n",
            "\tval loss: 35.65185\n",
            "\tepoch time: 31.434\n",
            "15th epoch,\n",
            "\ttraining loss: 5.85457\n",
            "\tval loss: 43.27762\n",
            "\tepoch time: 31.430\n",
            "16th epoch,\n",
            "\ttraining loss: 6.37123\n",
            "\tval loss: 40.17464\n",
            "\tepoch time: 31.375\n",
            "17th epoch,\n",
            "\ttraining loss: 19.71643\n",
            "\tval loss: 55.42248\n",
            "\tepoch time: 31.393\n",
            "18th epoch,\n",
            "\ttraining loss: 17.21197\n",
            "\tval loss: 38.07812\n",
            "\tepoch time: 31.371\n",
            "19th epoch,\n",
            "\ttraining loss: 11.31233\n",
            "\tval loss: 44.10363\n",
            "\tepoch time: 31.414\n",
            "20th epoch,\n",
            "\ttraining loss: 22.28146\n",
            "\tval loss: 38.38062\n",
            "\tepoch time: 31.375\n",
            "21th epoch,\n",
            "\ttraining loss: 17.52818\n",
            "\tval loss: 37.96685\n",
            "\tepoch time: 31.378\n",
            "22th epoch,\n",
            "\ttraining loss: 8.55618\n",
            "\tval loss: 48.63259\n",
            "\tepoch time: 31.423\n",
            "23th epoch,\n",
            "\ttraining loss: 36.51215\n",
            "\tval loss: 38.41278\n",
            "\tepoch time: 31.369\n",
            "24th epoch,\n",
            "\ttraining loss: 8.78996\n",
            "\tval loss: 42.16212\n",
            "\tepoch time: 31.384\n",
            "25th epoch,\n",
            "\ttraining loss: 10.61247\n",
            "\tval loss: 60.98355\n",
            "\tepoch time: 31.343\n",
            "26th epoch,\n",
            "\ttraining loss: 6.01322\n",
            "\tval loss: 36.05423\n",
            "\tepoch time: 31.395\n",
            "27th epoch,\n",
            "\ttraining loss: 4.38969\n",
            "\tval loss: 34.95720\n",
            "\tepoch time: 31.379\n",
            "28th epoch,\n",
            "\ttraining loss: 4.77375\n",
            "\tval loss: 60.83264\n",
            "\tepoch time: 31.369\n",
            "29th epoch,\n",
            "\ttraining loss: 1741.13056\n",
            "\tval loss: 187.29334\n",
            "\tepoch time: 31.276\n",
            "30th epoch,\n",
            "\ttraining loss: 66.39891\n",
            "\tval loss: 58.79088\n",
            "\tepoch time: 31.214\n",
            "31th epoch,\n",
            "\ttraining loss: 19.03754\n",
            "\tval loss: 44.06550\n",
            "\tepoch time: 31.249\n",
            "32th epoch,\n",
            "\ttraining loss: 11.67380\n",
            "\tval loss: 41.57095\n",
            "\tepoch time: 31.237\n",
            "33th epoch,\n",
            "\ttraining loss: 10.36368\n",
            "\tval loss: 45.16533\n",
            "\tepoch time: 31.244\n",
            "34th epoch,\n",
            "\ttraining loss: 7.94119\n",
            "\tval loss: 40.85518\n",
            "\tepoch time: 31.203\n",
            "35th epoch,\n",
            "\ttraining loss: 4.53870\n",
            "\tval loss: 36.00249\n",
            "\tepoch time: 31.244\n",
            "36th epoch,\n",
            "\ttraining loss: 3.24738\n",
            "\tval loss: 35.94262\n",
            "\tepoch time: 31.226\n",
            "37th epoch,\n",
            "\ttraining loss: 2.67752\n",
            "\tval loss: 35.64743\n",
            "\tepoch time: 31.183\n",
            "38th epoch,\n",
            "\ttraining loss: 3.25143\n",
            "\tval loss: 35.47466\n",
            "\tepoch time: 31.180\n",
            "39th epoch,\n",
            "\ttraining loss: 2.30614\n",
            "\tval loss: 35.35392\n",
            "\tepoch time: 31.258\n",
            "40th epoch,\n",
            "\ttraining loss: 2.46634\n",
            "\tval loss: 36.46057\n",
            "\tepoch time: 31.230\n",
            "41th epoch,\n",
            "\ttraining loss: 3.07263\n",
            "\tval loss: 36.55334\n",
            "\tepoch time: 31.225\n",
            "42th epoch,\n",
            "\ttraining loss: 2.45871\n",
            "\tval loss: 35.37172\n",
            "\tepoch time: 31.238\n",
            "43th epoch,\n",
            "\ttraining loss: 3.05962\n",
            "\tval loss: 35.81524\n",
            "\tepoch time: 31.213\n",
            "44th epoch,\n",
            "\ttraining loss: 2.99849\n",
            "\tval loss: 35.47984\n",
            "\tepoch time: 31.193\n",
            "45th epoch,\n",
            "\ttraining loss: 4.65434\n",
            "\tval loss: 36.31902\n",
            "\tepoch time: 31.785\n",
            "46th epoch,\n",
            "\ttraining loss: 4.71637\n",
            "\tval loss: 34.81223\n",
            "\tepoch time: 31.211\n",
            "47th epoch,\n",
            "\ttraining loss: 4.78600\n",
            "\tval loss: 36.53740\n",
            "\tepoch time: 31.813\n",
            "48th epoch,\n",
            "\ttraining loss: 4.58401\n",
            "\tval loss: 35.21113\n",
            "\tepoch time: 31.257\n",
            "49th epoch,\n",
            "\ttraining loss: 4.91165\n",
            "\tval loss: 35.51017\n",
            "\tepoch time: 31.835\n",
            "50th epoch,\n",
            "\ttraining loss: 6.18529\n",
            "\tval loss: 43.35629\n",
            "\tepoch time: 31.295\n",
            "----------\n",
            "Train Finished.\n",
            "Training time: 1574.96s\n",
            "The best epoch: 46\n",
            "The best val loss: 34.812234375\n"
          ]
        }
      ],
      "source": [
        "# 1. Load data and preprocessing\n",
        "file_path = 'assignment_2_smiles.txt'\n",
        "with open(file_path, 'r') as f:\n",
        "    smi_list = [l.strip() for l in f.readlines()]\n",
        "\n",
        "# Define Dataset and Dataloader\n",
        "dataset = Prob1Dataset(smi_list)\n",
        "train_dataset, valid_dataset, _ = \\\n",
        "        random_splitter(dataset, 0.9, 0.1, 0.0)\n",
        "len_max = dataset.max_length\n",
        "char_len = dataset.vec_dim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "data_loaders = {}\n",
        "data_loaders['train'] = DataLoader(train_dataset, batch_size=128, shuffle = True)\n",
        "data_loaders['val'] = DataLoader(valid_dataset, batch_size=128, shuffle = False)\n",
        "\n",
        "# 2. Train model\n",
        "# Setting learning parameters\n",
        "num_epoch = 50\n",
        "loss_fn = nn.MSELoss(reduction='sum')\n",
        "lr = 1e-4\n",
        "\n",
        "# prepare model\n",
        "model_architecture = 'Linear'\n",
        "#model_architecture = 'Conv'\n",
        "\n",
        "if model_architecture == 'Linear':\n",
        "    model = LinearRegressor(embedding_dim=64, num_hidden_layers=10, n_char=char_len, max_len=len_max)\n",
        "elif model_architecture == 'Conv':\n",
        "    model = ConvRegressor(n_channel=64, num_conv_layers=10, kernel_size=3, n_char=char_len, max_len=len_max)\n",
        "print(f'{model_architecture} model was used.')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Model training\n",
        "import time, copy\n",
        "model.cuda()\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "best_val_loss = 1e6\n",
        "train_start = time.time()\n",
        "for i in range(num_epoch):\n",
        "    since = time.time()\n",
        "    # 2-1. Training phase\n",
        "    model.train()\n",
        "    train_loss_list = []\n",
        "    for batch_idx, batch in enumerate(data_loaders['train']):\n",
        "        ###############################\n",
        "        ######## Implement Here #######\n",
        "        ###############################\n",
        "        x_batch = batch[\"input\"].float().to(device)\n",
        "        y_batch = batch[\"output\"].float().to(device)\n",
        "\n",
        "        y_pred = model(x_batch)\n",
        "\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        train_loss_list.append(copy.deepcopy(loss.data.cpu().numpy()))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_train_avg_loss = \\\n",
        "            np.sum(np.array(train_loss_list))/len(data_loaders['train'].dataset)\n",
        "    train_loss_history.append(epoch_train_avg_loss)\n",
        "\n",
        "    # 2-2. Validation phase\n",
        "    model.eval()\n",
        "    val_loss_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(data_loaders['val']):\n",
        "            ###############################\n",
        "            ######## Implement Here #######\n",
        "            ###############################\n",
        "            x_batch = batch[\"input\"].float().to(device)\n",
        "            y_batch = batch[\"output\"].float().to(device)\n",
        "\n",
        "            y_pred = model(x_batch)\n",
        "\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            val_loss_list.append(loss.data.cpu().numpy())\n",
        "\n",
        "    epoch_val_avg_loss = \\\n",
        "            np.sum(np.array(val_loss_list))/len(data_loaders['val'].dataset)\n",
        "    val_loss_history.append(epoch_val_avg_loss)\n",
        "\n",
        "    if epoch_val_avg_loss < best_val_loss:\n",
        "        best_epoch = i+1\n",
        "        best_val_loss = epoch_val_avg_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # 2-3. print the result\n",
        "    end = time.time()\n",
        "    print(f'{i+1}th epoch,')\n",
        "    print(f'\\ttraining loss: {epoch_train_avg_loss:.5f}')\n",
        "    print(f'\\tval loss: {epoch_val_avg_loss:.5f}')\n",
        "    print(f'\\tepoch time: {end-since:.3f}')\n",
        "\n",
        "save_dir = 'assignment_2_models'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "torch.save(best_model_wts, f'{save_dir}/Best_{model_architecture}_{str(best_epoch)}.pt')\n",
        "print('-'*10)\n",
        "print('Train Finished.')\n",
        "print(f'Training time: {time.time()-train_start:.2f}s')\n",
        "print(f'The best epoch: {best_epoch}')\n",
        "print(f'The best val loss: {best_val_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "L6kGu3QESp9C",
        "outputId": "f104d9ff-fadb-41a5-8f5b-dd6d6eb14955"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x_axis = np.arange(num_epoch)\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(x_axis, train_loss_history, label='train loss')\n",
        "ax.plot(x_axis, val_loss_history, label='val loss')\n",
        "ax.set_xlabel('num epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Loss History')\n",
        "ax.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3fwJpCnXlzE",
        "outputId": "a7a772b2-ab3d-4492-c6ca-be7bc1568ec7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('assignment_2_models/Best_Linear_46.pt', map_location=\"cuda:0\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU5cybhNYvpj",
        "outputId": "f51edee0-41e4-4720-de00-ca1fdc051362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hidden_layers.0.weight: torch.Size([6144, 6144])\n",
            "hidden_layers.0.bias: torch.Size([6144])\n",
            "hidden_layers.1.weight: torch.Size([6144, 6144])\n",
            "hidden_layers.1.bias: torch.Size([6144])\n",
            "hidden_layers.2.weight: torch.Size([6144, 6144])\n",
            "hidden_layers.2.bias: torch.Size([6144])\n",
            "hidden_layers.3.weight: torch.Size([6144, 6144])\n",
            "hidden_layers.3.bias: torch.Size([6144])\n",
            "hidden_layers.4.weight: torch.Size([6144, 6144])\n",
            "hidden_layers.4.bias: torch.Size([6144])\n",
            "hidden_layers.5.weight: torch.Size([6144, 6144])\n",
            "hidden_layers.5.bias: torch.Size([6144])\n",
            "hidden_layers.6.weight: torch.Size([6144, 6144])\n",
            "hidden_layers.6.bias: torch.Size([6144])\n",
            "hidden_layers.7.weight: torch.Size([6144, 6144])\n",
            "hidden_layers.7.bias: torch.Size([6144])\n",
            "hidden_layers.8.weight: torch.Size([6144, 6144])\n",
            "hidden_layers.8.bias: torch.Size([6144])\n",
            "hidden_layers.9.weight: torch.Size([6144, 6144])\n",
            "hidden_layers.9.bias: torch.Size([6144])\n",
            "fc.weight: torch.Size([1, 6144])\n",
            "fc.bias: torch.Size([1])\n",
            "embedding.weight: torch.Size([64, 37])\n"
          ]
        }
      ],
      "source": [
        "for key in model.state_dict().keys():\n",
        "    print(f\"{key}: {model.state_dict()[key].size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnjGfW6EZ-p0",
        "outputId": "b6792042-afa3-44f4-8dd9-7e502b4167e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ConvRegressor(n_channel=64, num_conv_layers=10, kernel_size=3, n_char=char_len, max_len=len_max)\n",
        "model.load_state_dict(torch.load('assignment_2_models/Best_Conv_50.pt', map_location=\"cuda:0\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yozfZbw1aIjE",
        "outputId": "629e22aa-902d-492d-c286-231cc6c79965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv_layers.0.weight: torch.Size([64, 64, 3])\n",
            "conv_layers.0.bias: torch.Size([64])\n",
            "conv_layers.1.weight: torch.Size([64, 64, 3])\n",
            "conv_layers.1.bias: torch.Size([64])\n",
            "conv_layers.2.weight: torch.Size([64, 64, 3])\n",
            "conv_layers.2.bias: torch.Size([64])\n",
            "conv_layers.3.weight: torch.Size([64, 64, 3])\n",
            "conv_layers.3.bias: torch.Size([64])\n",
            "conv_layers.4.weight: torch.Size([64, 64, 3])\n",
            "conv_layers.4.bias: torch.Size([64])\n",
            "conv_layers.5.weight: torch.Size([64, 64, 3])\n",
            "conv_layers.5.bias: torch.Size([64])\n",
            "conv_layers.6.weight: torch.Size([64, 64, 3])\n",
            "conv_layers.6.bias: torch.Size([64])\n",
            "conv_layers.7.weight: torch.Size([64, 64, 3])\n",
            "conv_layers.7.bias: torch.Size([64])\n",
            "conv_layers.8.weight: torch.Size([64, 64, 3])\n",
            "conv_layers.8.bias: torch.Size([64])\n",
            "conv_layers.9.weight: torch.Size([64, 64, 3])\n",
            "conv_layers.9.bias: torch.Size([64])\n",
            "fc.weight: torch.Size([1, 6144])\n",
            "fc.bias: torch.Size([1])\n",
            "embedding.weight: torch.Size([64, 37])\n"
          ]
        }
      ],
      "source": [
        "for key in model.state_dict().keys():\n",
        "    print(f\"{key}: {model.state_dict()[key].size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KvgA8ddX7tw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "649aKOamUxhW"
      },
      "source": [
        "# Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ojvGeoSU1Z4"
      },
      "outputs": [],
      "source": [
        "from rdkit.Chem.FragmentMatcher import FragmentMatcher\n",
        "\n",
        "\n",
        "class Prob2Dataset(Prob1Dataset):\n",
        "\n",
        "    def __init__(self, smi_list):\n",
        "        super().__init__(smi_list)\n",
        "\n",
        "        self.smi_list = [self._kekulize_smi(s) for s in smi_list]\n",
        "\n",
        "        ##########Implement Here!##########\n",
        "        '''\n",
        "        Get the longest length of smiles in the given smi_list\n",
        "        '''\n",
        "        self.max_length = len(max(self.smi_list, key = len))\n",
        "        ###################################\n",
        "\n",
        "        self._set_c_to_i()\n",
        "        self.vec_dim = self._get_num_char()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        return a dict of {\"input\": input, \"output\": output},\n",
        "        where input is a numpy array of encoded smiles and\n",
        "        output is a numpy array of corresponding label.\n",
        "        use self._encode_smi and self._has_benzene_substruct.\n",
        "        '''\n",
        "        sample = dict()\n",
        "        ##########Implement Here!##########\n",
        "        sample = {\n",
        "                \"input\": self._encode_smi(self.smi_list[idx]),\n",
        "                \"output\": self._has_benzene_substruct(self.smi_list[idx])\n",
        "        }\n",
        "        ###################################\n",
        "        return sample\n",
        "\n",
        "    def _kekulize_smi(self, smi):\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        Chem.Kekulize(mol)\n",
        "        ksmi = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
        "        return ksmi\n",
        "\n",
        "    def _has_benzene_substruct(self, smi):\n",
        "        FRAGMENT = \"c1ccccc1\"\n",
        "        matcher = FragmentMatcher()\n",
        "        matcher.Init(FRAGMENT)\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if matcher.HasMatch(mol):\n",
        "            return 1\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EYz4okcU_QP"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ConvClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, n_channel=128, num_conv_layers=5, padding=1, kernel_size=3, stride=1, n_char=37, fc_dim=None):\n",
        "        super(ConvClassifier, self).__init__()\n",
        "        print(f\"model's num_conv_layers : {num_conv_layers}\")\n",
        "        print(f\"model's padding : {padding}\")\n",
        "        print(f\"model's kernel_size : {kernel_size}\")\n",
        "        print(f\"model's stride : {stride}\")\n",
        "        ###############################\n",
        "        ######## Implement Here #######\n",
        "        ###############################\n",
        "        self.n_channel = n_channel\n",
        "        self.num_conv_layers = num_conv_layers\n",
        "        self.padding = padding\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.n_char = n_char\n",
        "        self.fc_dim = fc_dim\n",
        "        self.max_len = max_len\n",
        "\n",
        "        conv_layers = []\n",
        "        for _ in range(num_conv_layers):\n",
        "            conv_layers.append(\n",
        "                nn.Conv1d(\n",
        "                    n_channel,\n",
        "                    n_channel,\n",
        "                    kernel_size,\n",
        "                    stride,\n",
        "                    padding\n",
        "                ) # in_channels, out_channels, kernel_size, stride, padding\n",
        "            )\n",
        "\n",
        "        self.conv_layers = nn.ModuleList(conv_layers)\n",
        "        self.fc = nn.Linear(n_channel*(max_len+stride)//8, fc_dim)\n",
        "        self.embedding = nn.Linear(n_char, n_channel, bias=False)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        retval = None\n",
        "        ###############################\n",
        "        ######## Implement Here #######\n",
        "        ###############################\n",
        "        retval = x                                # [B x max_len x n_char]\n",
        "        # Embedding\n",
        "        retval = self.embedding(retval)           # [B x max_len x n_channel]\n",
        "        retval = retval.permute((0, 2, 1))        # [B x n_channel x max_len]\n",
        "        # Convolution and activation\n",
        "        for layer in self.conv_layers:\n",
        "            retval = layer(retval)\n",
        "            retval = self.activation(retval)\n",
        "        retval = retval.view(retval.size(0), -1)\n",
        "        retval = self.fc(retval)                  # [B x 1]\n",
        "        return retval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFnO4VjBVCnv",
        "outputId": "bbab7aef-fa8f-4906-c2a5-39abfa94c5f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model's num_conv_layers : 3\n",
            "model's padding : 1\n",
            "model's kernel_size : 3\n",
            "model's stride : 2\n",
            "ConvClassifier model was used.\n"
          ]
        }
      ],
      "source": [
        "# 1. Load data and preprocessing\n",
        "file_path = 'assignment_2_smiles.txt'\n",
        "with open(file_path, 'r') as f:\n",
        "    smi_list = [l.strip() for l in f.readlines()]\n",
        "\n",
        "# Define Dataset and Dataloader\n",
        "dataset = Prob2Dataset(smi_list)\n",
        "train_dataset, valid_dataset, _ = \\\n",
        "        random_splitter(dataset, 0.9, 0.1, 0.0)\n",
        "max_len = dataset.max_length\n",
        "char_len = dataset.vec_dim\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "data_loaders = {}\n",
        "data_loaders['train'] = DataLoader(train_dataset, batch_size=128, shuffle = True)\n",
        "data_loaders['val'] = DataLoader(valid_dataset, batch_size=128, shuffle = False)\n",
        "\n",
        "# 2. Train model\n",
        "# Setting learning parameters\n",
        "num_epoch = 100\n",
        "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "lr = 1e-4\n",
        "gamma = 0.99\n",
        "n_channel = 128\n",
        "\n",
        "# prepare model\n",
        "#model = ConvClassifier(n_channel=n_channel, num_conv_layers=3, padding=1, kernel_size=3, stride=1, n_char=char_len, fc_dim = 2)  # Model 1\n",
        "#model = ConvClassifier(n_channel=n_channel, num_conv_layers=3, padding=2, kernel_size=5, stride=1, n_char=char_len, fc_dim = 2)  # Model 2\n",
        "model = ConvClassifier(n_channel=n_channel, num_conv_layers=3, padding=1, kernel_size=3, stride=2, n_char=char_len, fc_dim = 2)  # Model 3\n",
        "\n",
        "print('ConvClassifier model was used.')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evnzSnofVGDq",
        "outputId": "ecaac430-9ae7-4a4f-81b9-539d01b0169e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1th epoch,\n",
            "\ttraining loss: 0.42757\n",
            "\tval loss: 0.30519\n",
            "\tepoch time: 10.980\n",
            "2th epoch,\n",
            "\ttraining loss: 0.26991\n",
            "\tval loss: 0.22067\n",
            "\tepoch time: 10.894\n",
            "3th epoch,\n",
            "\ttraining loss: 0.16254\n",
            "\tval loss: 0.14060\n",
            "\tepoch time: 12.345\n",
            "4th epoch,\n",
            "\ttraining loss: 0.11218\n",
            "\tval loss: 0.10694\n",
            "\tepoch time: 12.412\n",
            "5th epoch,\n",
            "\ttraining loss: 0.09398\n",
            "\tval loss: 0.09400\n",
            "\tepoch time: 10.883\n",
            "6th epoch,\n",
            "\ttraining loss: 0.08008\n",
            "\tval loss: 0.08912\n",
            "\tepoch time: 10.786\n",
            "7th epoch,\n",
            "\ttraining loss: 0.06980\n",
            "\tval loss: 0.07809\n",
            "\tepoch time: 11.305\n",
            "8th epoch,\n",
            "\ttraining loss: 0.06060\n",
            "\tval loss: 0.06940\n",
            "\tepoch time: 10.762\n",
            "9th epoch,\n",
            "\ttraining loss: 0.05496\n",
            "\tval loss: 0.06379\n",
            "\tepoch time: 12.248\n",
            "10th epoch,\n",
            "\ttraining loss: 0.04937\n",
            "\tval loss: 0.08535\n",
            "\tepoch time: 11.389\n",
            "11th epoch,\n",
            "\ttraining loss: 0.04708\n",
            "\tval loss: 0.06508\n",
            "\tepoch time: 10.881\n",
            "12th epoch,\n",
            "\ttraining loss: 0.04380\n",
            "\tval loss: 0.05631\n",
            "\tepoch time: 11.219\n",
            "13th epoch,\n",
            "\ttraining loss: 0.04034\n",
            "\tval loss: 0.05401\n",
            "\tepoch time: 10.791\n",
            "14th epoch,\n",
            "\ttraining loss: 0.03790\n",
            "\tval loss: 0.05265\n",
            "\tepoch time: 12.381\n",
            "15th epoch,\n",
            "\ttraining loss: 0.03606\n",
            "\tval loss: 0.05161\n",
            "\tepoch time: 10.924\n",
            "16th epoch,\n",
            "\ttraining loss: 0.03447\n",
            "\tval loss: 0.05240\n",
            "\tepoch time: 10.850\n",
            "17th epoch,\n",
            "\ttraining loss: 0.03260\n",
            "\tval loss: 0.05106\n",
            "\tepoch time: 10.832\n",
            "18th epoch,\n",
            "\ttraining loss: 0.03119\n",
            "\tval loss: 0.04981\n",
            "\tepoch time: 11.055\n",
            "19th epoch,\n",
            "\ttraining loss: 0.03039\n",
            "\tval loss: 0.05071\n",
            "\tepoch time: 12.075\n",
            "20th epoch,\n",
            "\ttraining loss: 0.02870\n",
            "\tval loss: 0.05069\n",
            "\tepoch time: 10.877\n",
            "21th epoch,\n",
            "\ttraining loss: 0.02765\n",
            "\tval loss: 0.04914\n",
            "\tepoch time: 10.601\n",
            "22th epoch,\n",
            "\ttraining loss: 0.02703\n",
            "\tval loss: 0.04873\n",
            "\tepoch time: 12.307\n",
            "23th epoch,\n",
            "\ttraining loss: 0.02587\n",
            "\tval loss: 0.04827\n",
            "\tepoch time: 12.407\n",
            "24th epoch,\n",
            "\ttraining loss: 0.02513\n",
            "\tval loss: 0.04663\n",
            "\tepoch time: 10.698\n",
            "25th epoch,\n",
            "\ttraining loss: 0.02414\n",
            "\tval loss: 0.04717\n",
            "\tepoch time: 10.700\n",
            "26th epoch,\n",
            "\ttraining loss: 0.02343\n",
            "\tval loss: 0.04463\n",
            "\tepoch time: 10.724\n",
            "27th epoch,\n",
            "\ttraining loss: 0.02176\n",
            "\tval loss: 0.04910\n",
            "\tepoch time: 10.818\n",
            "28th epoch,\n",
            "\ttraining loss: 0.02218\n",
            "\tval loss: 0.04669\n",
            "\tepoch time: 10.683\n",
            "29th epoch,\n",
            "\ttraining loss: 0.02112\n",
            "\tval loss: 0.04559\n",
            "\tepoch time: 10.780\n",
            "30th epoch,\n",
            "\ttraining loss: 0.02016\n",
            "\tval loss: 0.05031\n",
            "\tepoch time: 10.596\n",
            "31th epoch,\n",
            "\ttraining loss: 0.01865\n",
            "\tval loss: 0.04314\n",
            "\tepoch time: 10.721\n",
            "32th epoch,\n",
            "\ttraining loss: 0.01891\n",
            "\tval loss: 0.04417\n",
            "\tepoch time: 10.694\n",
            "33th epoch,\n",
            "\ttraining loss: 0.01800\n",
            "\tval loss: 0.04207\n",
            "\tepoch time: 11.481\n",
            "34th epoch,\n",
            "\ttraining loss: 0.01689\n",
            "\tval loss: 0.04544\n",
            "\tepoch time: 11.704\n",
            "35th epoch,\n",
            "\ttraining loss: 0.01788\n",
            "\tval loss: 0.04177\n",
            "\tepoch time: 10.841\n",
            "36th epoch,\n",
            "\ttraining loss: 0.01614\n",
            "\tval loss: 0.04410\n",
            "\tepoch time: 10.816\n",
            "37th epoch,\n",
            "\ttraining loss: 0.01560\n",
            "\tval loss: 0.04793\n",
            "\tepoch time: 10.802\n",
            "38th epoch,\n",
            "\ttraining loss: 0.01533\n",
            "\tval loss: 0.04034\n",
            "\tepoch time: 12.317\n",
            "39th epoch,\n",
            "\ttraining loss: 0.01470\n",
            "\tval loss: 0.04424\n",
            "\tepoch time: 11.000\n",
            "40th epoch,\n",
            "\ttraining loss: 0.01419\n",
            "\tval loss: 0.04200\n",
            "\tepoch time: 10.907\n",
            "41th epoch,\n",
            "\ttraining loss: 0.01438\n",
            "\tval loss: 0.04914\n",
            "\tepoch time: 12.283\n",
            "42th epoch,\n",
            "\ttraining loss: 0.01410\n",
            "\tval loss: 0.04254\n",
            "\tepoch time: 10.962\n",
            "43th epoch,\n",
            "\ttraining loss: 0.01380\n",
            "\tval loss: 0.05173\n",
            "\tepoch time: 12.268\n",
            "44th epoch,\n",
            "\ttraining loss: 0.01269\n",
            "\tval loss: 0.03934\n",
            "\tepoch time: 10.876\n",
            "45th epoch,\n",
            "\ttraining loss: 0.01229\n",
            "\tval loss: 0.04116\n",
            "\tepoch time: 10.842\n",
            "46th epoch,\n",
            "\ttraining loss: 0.01188\n",
            "\tval loss: 0.04093\n",
            "\tepoch time: 10.757\n",
            "47th epoch,\n",
            "\ttraining loss: 0.01160\n",
            "\tval loss: 0.04411\n",
            "\tepoch time: 11.529\n",
            "48th epoch,\n",
            "\ttraining loss: 0.01174\n",
            "\tval loss: 0.04517\n",
            "\tepoch time: 11.756\n",
            "49th epoch,\n",
            "\ttraining loss: 0.01171\n",
            "\tval loss: 0.04006\n",
            "\tepoch time: 11.346\n",
            "50th epoch,\n",
            "\ttraining loss: 0.01062\n",
            "\tval loss: 0.03851\n",
            "\tepoch time: 10.824\n",
            "51th epoch,\n",
            "\ttraining loss: 0.01137\n",
            "\tval loss: 0.04329\n",
            "\tepoch time: 10.766\n",
            "52th epoch,\n",
            "\ttraining loss: 0.01211\n",
            "\tval loss: 0.04318\n",
            "\tepoch time: 12.216\n",
            "53th epoch,\n",
            "\ttraining loss: 0.00975\n",
            "\tval loss: 0.05613\n",
            "\tepoch time: 10.701\n",
            "54th epoch,\n",
            "\ttraining loss: 0.01028\n",
            "\tval loss: 0.05226\n",
            "\tepoch time: 10.671\n",
            "55th epoch,\n",
            "\ttraining loss: 0.00960\n",
            "\tval loss: 0.04005\n",
            "\tepoch time: 10.762\n",
            "56th epoch,\n",
            "\ttraining loss: 0.00889\n",
            "\tval loss: 0.03988\n",
            "\tepoch time: 10.743\n",
            "57th epoch,\n",
            "\ttraining loss: 0.00923\n",
            "\tval loss: 0.04289\n",
            "\tepoch time: 12.189\n",
            "58th epoch,\n",
            "\ttraining loss: 0.00886\n",
            "\tval loss: 0.03553\n",
            "\tepoch time: 10.647\n",
            "59th epoch,\n",
            "\ttraining loss: 0.00840\n",
            "\tval loss: 0.03785\n",
            "\tepoch time: 11.343\n",
            "60th epoch,\n",
            "\ttraining loss: 0.00807\n",
            "\tval loss: 0.03902\n",
            "\tepoch time: 11.586\n",
            "61th epoch,\n",
            "\ttraining loss: 0.00795\n",
            "\tval loss: 0.03513\n",
            "\tepoch time: 10.856\n",
            "62th epoch,\n",
            "\ttraining loss: 0.00811\n",
            "\tval loss: 0.04064\n",
            "\tepoch time: 12.305\n",
            "63th epoch,\n",
            "\ttraining loss: 0.00743\n",
            "\tval loss: 0.03853\n",
            "\tepoch time: 10.872\n",
            "64th epoch,\n",
            "\ttraining loss: 0.00740\n",
            "\tval loss: 0.05691\n",
            "\tepoch time: 10.834\n",
            "65th epoch,\n",
            "\ttraining loss: 0.00774\n",
            "\tval loss: 0.05125\n",
            "\tepoch time: 10.605\n",
            "66th epoch,\n",
            "\ttraining loss: 0.00785\n",
            "\tval loss: 0.03578\n",
            "\tepoch time: 11.681\n",
            "67th epoch,\n",
            "\ttraining loss: 0.00740\n",
            "\tval loss: 0.04353\n",
            "\tepoch time: 11.437\n",
            "68th epoch,\n",
            "\ttraining loss: 0.00662\n",
            "\tval loss: 0.03871\n",
            "\tepoch time: 11.080\n",
            "69th epoch,\n",
            "\ttraining loss: 0.00674\n",
            "\tval loss: 0.04085\n",
            "\tepoch time: 10.873\n",
            "70th epoch,\n",
            "\ttraining loss: 0.00614\n",
            "\tval loss: 0.03885\n",
            "\tepoch time: 10.924\n",
            "71th epoch,\n",
            "\ttraining loss: 0.00655\n",
            "\tval loss: 0.03403\n",
            "\tepoch time: 12.451\n",
            "72th epoch,\n",
            "\ttraining loss: 0.00599\n",
            "\tval loss: 0.03431\n",
            "\tepoch time: 10.870\n",
            "73th epoch,\n",
            "\ttraining loss: 0.00661\n",
            "\tval loss: 0.04393\n",
            "\tepoch time: 10.913\n",
            "74th epoch,\n",
            "\ttraining loss: 0.00564\n",
            "\tval loss: 0.04018\n",
            "\tepoch time: 10.823\n",
            "75th epoch,\n",
            "\ttraining loss: 0.00611\n",
            "\tval loss: 0.03775\n",
            "\tepoch time: 10.893\n",
            "76th epoch,\n",
            "\ttraining loss: 0.00540\n",
            "\tval loss: 0.03705\n",
            "\tepoch time: 12.354\n",
            "77th epoch,\n",
            "\ttraining loss: 0.00629\n",
            "\tval loss: 0.03417\n",
            "\tepoch time: 10.922\n",
            "78th epoch,\n",
            "\ttraining loss: 0.00588\n",
            "\tval loss: 0.04727\n",
            "\tepoch time: 12.334\n",
            "79th epoch,\n",
            "\ttraining loss: 0.00572\n",
            "\tval loss: 0.03899\n",
            "\tepoch time: 10.755\n",
            "80th epoch,\n",
            "\ttraining loss: 0.00473\n",
            "\tval loss: 0.03780\n",
            "\tepoch time: 12.315\n",
            "81th epoch,\n",
            "\ttraining loss: 0.00497\n",
            "\tval loss: 0.03354\n",
            "\tepoch time: 10.745\n",
            "82th epoch,\n",
            "\ttraining loss: 0.00481\n",
            "\tval loss: 0.03986\n",
            "\tepoch time: 10.847\n",
            "83th epoch,\n",
            "\ttraining loss: 0.00460\n",
            "\tval loss: 0.03559\n",
            "\tepoch time: 10.745\n",
            "84th epoch,\n",
            "\ttraining loss: 0.00471\n",
            "\tval loss: 0.03774\n",
            "\tepoch time: 10.906\n",
            "85th epoch,\n",
            "\ttraining loss: 0.00466\n",
            "\tval loss: 0.03579\n",
            "\tepoch time: 12.317\n",
            "86th epoch,\n",
            "\ttraining loss: 0.00443\n",
            "\tval loss: 0.03859\n",
            "\tepoch time: 10.714\n",
            "87th epoch,\n",
            "\ttraining loss: 0.00448\n",
            "\tval loss: 0.04234\n",
            "\tepoch time: 10.817\n",
            "88th epoch,\n",
            "\ttraining loss: 0.00411\n",
            "\tval loss: 0.04318\n",
            "\tepoch time: 10.914\n",
            "89th epoch,\n",
            "\ttraining loss: 0.00389\n",
            "\tval loss: 0.04284\n",
            "\tepoch time: 10.741\n",
            "90th epoch,\n",
            "\ttraining loss: 0.00416\n",
            "\tval loss: 0.03373\n",
            "\tepoch time: 12.250\n",
            "91th epoch,\n",
            "\ttraining loss: 0.00414\n",
            "\tval loss: 0.04016\n",
            "\tepoch time: 10.799\n",
            "92th epoch,\n",
            "\ttraining loss: 0.00370\n",
            "\tval loss: 0.04010\n",
            "\tepoch time: 10.703\n",
            "93th epoch,\n",
            "\ttraining loss: 0.00345\n",
            "\tval loss: 0.03142\n",
            "\tepoch time: 10.757\n",
            "94th epoch,\n",
            "\ttraining loss: 0.00380\n",
            "\tval loss: 0.04031\n",
            "\tepoch time: 12.597\n",
            "95th epoch,\n",
            "\ttraining loss: 0.00334\n",
            "\tval loss: 0.03339\n",
            "\tepoch time: 11.475\n",
            "96th epoch,\n",
            "\ttraining loss: 0.00350\n",
            "\tval loss: 0.03887\n",
            "\tepoch time: 10.704\n",
            "97th epoch,\n",
            "\ttraining loss: 0.00396\n",
            "\tval loss: 0.06192\n",
            "\tepoch time: 12.276\n",
            "98th epoch,\n",
            "\ttraining loss: 0.00353\n",
            "\tval loss: 0.03883\n",
            "\tepoch time: 10.923\n",
            "99th epoch,\n",
            "\ttraining loss: 0.00343\n",
            "\tval loss: 0.03379\n",
            "\tepoch time: 12.683\n",
            "100th epoch,\n",
            "\ttraining loss: 0.00315\n",
            "\tval loss: 0.03454\n",
            "\tepoch time: 10.802\n",
            "----------\n",
            "Train Finished.\n",
            "Training time: 1123.41s\n",
            "The best epoch: 93\n",
            "The best val loss: 0.031417278289794924\n"
          ]
        }
      ],
      "source": [
        "# Model training\n",
        "import time, copy\n",
        "model.cuda()\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "best_val_loss = 1e6\n",
        "train_start = time.time()\n",
        "for i in range(num_epoch):\n",
        "    since = time.time()\n",
        "    # 2-1. Training phase\n",
        "    model.train()\n",
        "    train_loss_list = []\n",
        "    for batch_idx, batch in enumerate(data_loaders['train']):\n",
        "        ###############################\n",
        "        ######## Implement Here #######\n",
        "        ###############################\n",
        "        x_batch = batch[\"input\"].float().to(device)\n",
        "        y_batch = batch[\"output\"].long().to(device)\n",
        "\n",
        "        y_pred = model(x_batch)\n",
        "\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        train_loss_list.append(copy.deepcopy(loss.data.cpu().numpy()))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_train_avg_loss = \\\n",
        "            np.sum(np.array(train_loss_list))/len(data_loaders['train'].dataset)\n",
        "    train_loss_history.append(epoch_train_avg_loss)\n",
        "\n",
        "    # 2-2. Validation phase\n",
        "    model.eval()\n",
        "    val_loss_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(data_loaders['val']):\n",
        "            ###############################\n",
        "            ######## Implement Here #######\n",
        "            ###############################\n",
        "            x_batch = batch[\"input\"].float().to(device)\n",
        "            y_batch = batch[\"output\"].long().to(device)\n",
        "\n",
        "            y_pred = model(x_batch)\n",
        "\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            val_loss_list.append(loss.data.cpu().numpy())\n",
        "\n",
        "    epoch_val_avg_loss = \\\n",
        "            np.sum(np.array(val_loss_list))/len(data_loaders['val'].dataset)\n",
        "    val_loss_history.append(epoch_val_avg_loss)\n",
        "\n",
        "    if epoch_val_avg_loss < best_val_loss:\n",
        "        best_epoch = i+1\n",
        "        best_val_loss = epoch_val_avg_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # 2-3. print the result\n",
        "    end = time.time()\n",
        "    print(f'{i+1}th epoch,')\n",
        "    print(f'\\ttraining loss: {epoch_train_avg_loss:.5f}')\n",
        "    print(f'\\tval loss: {epoch_val_avg_loss:.5f}')\n",
        "    print(f'\\tepoch time: {end-since:.3f}')\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "save_dir = 'assignment_2_models'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "torch.save(best_model_wts, f'{save_dir}/Best_ConvClassifier_{str(best_epoch)}.pt')\n",
        "print('-'*10)\n",
        "print('Train Finished.')\n",
        "print(f'Training time: {time.time()-train_start:.2f}s')\n",
        "print(f'The best epoch: {best_epoch}')\n",
        "print(f'The best val loss: {best_val_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "tyQIA7XWVJEJ",
        "outputId": "7cab437f-edfd-416c-dd79-b3a8fbe4b410"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 4.Plot the loss histories\n",
        "import matplotlib.pyplot as plt\n",
        "x_axis = np.arange(num_epoch)\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(x_axis, train_loss_history, label='train loss')\n",
        "ax.plot(x_axis, val_loss_history, label='val loss')\n",
        "ax.set_xlabel('num epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Loss History')\n",
        "ax.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUhBVzX-BIie",
        "outputId": "91b7455e-87ac-4b50-e928-5852471ad215"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('assignment_2_models/Best_ConvClassifier_93.pt', map_location=\"cuda:0\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KleYwRz8qFc",
        "outputId": "059ed61d-fa00-45a1-8da6-256595948520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv_layers.0.weight: torch.Size([128, 128, 3])\n",
            "conv_layers.0.bias: torch.Size([128])\n",
            "conv_layers.1.weight: torch.Size([128, 128, 3])\n",
            "conv_layers.1.bias: torch.Size([128])\n",
            "conv_layers.2.weight: torch.Size([128, 128, 3])\n",
            "conv_layers.2.bias: torch.Size([128])\n",
            "fc.weight: torch.Size([2, 1664])\n",
            "fc.bias: torch.Size([2])\n",
            "embedding.weight: torch.Size([128, 34])\n"
          ]
        }
      ],
      "source": [
        "for key in model.state_dict().keys():\n",
        "    print(f\"{key}: {model.state_dict()[key].size()}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
